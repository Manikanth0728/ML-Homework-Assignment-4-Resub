{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUOgIeG9aBLY",
        "outputId": "d5e14383-feb4-4951-efc2-e68fbf8a6929"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered tokens (Word – Lemma – POS):\n",
            "enjoys     → enjoy      (VERB)\n",
            "playing    → play       (VERB)\n",
            "football   → football   (NOUN)\n",
            "reading    → read       (VERB)\n",
            "books      → book       (NOUN)\n",
            "library    → library    (NOUN)\n",
            "\n",
            "Lemmatized result: ['enjoy', 'play', 'football', 'read', 'book', 'library']\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------\n",
        "# Part C - Question 1 \n",
        "# Complete Text Preprocessing Flow\n",
        "# -------------------------------\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download required resources (first-time use)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "sentence = \"Machine learning models become more accurate when the data is cleaned and properly preprocessed.\"\n",
        "\n",
        "# Step 1: Convert sentence into tokens\n",
        "tokens_list = word_tokenize(sentence)\n",
        "\n",
        "# Step 2: Remove stopwords\n",
        "stopword_set = set(stopwords.words(\"english\"))\n",
        "tokens_no_stop = [tok for tok in tokens_list if tok.lower() not in stopword_set]\n",
        "\n",
        "# Step 3: Lemmatize each token\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(token) for token in tokens_no_stop]\n",
        "\n",
        "# Step 4: POS tagging → keep only nouns\n",
        "pos_info = pos_tag(lemmatized)\n",
        "nouns_only = [word for word, tag in pos_info if tag.startswith(\"NN\")]\n",
        "\n",
        "print(\"Original text:\", sentence)\n",
        "print(\"After tokenization:\", tokens_list)\n",
        "print(\"Without stopwords:\", tokens_no_stop)\n",
        "print(\"Lemmatized tokens:\", lemmatized)\n",
        "print(\"Filtered nouns:\", nouns_only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz1zYr9GadwP",
        "outputId": "f8de7aa3-7a44-4e8f-e705-4f39a09a4cd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\dmkr1\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\dmkr1\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\dmkr1\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\dmkr1\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: John met Alex after work, and he gave him the project files.\n",
            "Detected Persons: ['John', 'Alex']\n",
            "Detected Pronouns: ['he', 'him']\n",
            "\n",
            "Conclusion: The pronoun reference is AMBIGUOUS.\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------\n",
        "# Part C - Question 2 \n",
        "# Pronoun Ambiguity Detection using NLTK\n",
        "# --------------------------------------\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "# Make sure required NLTK data is available\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Input text\n",
        "text_input = \"John met Alex after work, and he gave him the project files.\"\n",
        "\n",
        "# Step 1: Tokenize\n",
        "tokens = word_tokenize(text_input)\n",
        "\n",
        "# Step 2: POS tagging\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "# Step 3: Named Entity Recognition (Tree format)\n",
        "ner_tree = ne_chunk(tagged)\n",
        "\n",
        "# Extract all PERSON names from NER\n",
        "person_names = []\n",
        "for subtree in ner_tree:\n",
        "    if hasattr(subtree, 'label') and subtree.label() == 'PERSON':\n",
        "        name = \" \".join([leaf[0] for leaf in subtree.leaves()])\n",
        "        person_names.append(name)\n",
        "\n",
        "# Pronouns indicating potential ambiguity\n",
        "pronoun_set = {\"he\", \"she\", \"they\", \"him\", \"her\"}\n",
        "\n",
        "# Detect pronouns in sentence\n",
        "found_prons = [tok.lower() for tok in tokens if tok.lower() in pronoun_set]\n",
        "\n",
        "print(\"Sentence:\", text_input)\n",
        "print(\"Detected Persons:\", person_names)\n",
        "print(\"Detected Pronouns:\", found_prons)\n",
        "\n",
        "# Ambiguity condition:\n",
        "# If there's more than one person AND any pronoun exists\n",
        "if len(person_names) > 1 and len(found_prons) > 0:\n",
        "    print(\"\\nConclusion: The pronoun reference is AMBIGUOUS.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: No pronoun ambiguity detected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBDRsDFCaSFH"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
